# 로지스틱 회귀

날짜: 2022-04-22

로지스틱 회귀는 선형 회귀 방식을 분류에 적용한 알고리즘으로 분류에 사용된다.

로지스틱 회귀가 선형 회귀와 다른 점은 학습을 통해 선형 함수의 회귀 최적선을 찾는 것이 아니라 시그모이드 함수 최적선을 찾고 이 시그모이드 함수의 반환 값을 확률로 가주해 확률에 따라 분류를 결정한다.

로지스틱 회귀에서 예측 값은 예측 확률을 의미하며, 예측 값 즉 예측 확률이 0.5 이상이면 1로 0.5 이하이면 0으로 예측한다. 로지스틱 회귀의 예측 확률으 ㄴ시그모이드 함수의 출력값으로 계산된다.

이진분류 예측 성능이 뛰어나 이진 분류의 기본모델로 사용하며 희소한 데이터 세트 분류에도 뛰어난 성능을 보여서 텍스트 분류에서도 자주 사용된다.

로지스틱regression 클래스의 C는 alpha값의 역수이다. C 값이 작을 수록 규제 강도가 크다.

```python
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression

cancer = load_breast_cancer()

from sklearn. preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

scaler = StandardScaler()
data_scaled = scaler.fit_transform(cancer.data)

X_train, X_test, y_train, y_test = train_test_split(data_scaled, cancer.target, test_size = 0.3, random_state=0)

from sklearn.metrics import accuracy_score, roc_auc_score
```

```python
lr_clf = LogisticRegression()
lr_clf.fit(X_train, y_train)
lr_preds= lr_clf.predict(X_test)
score = accuracy_score(y_test, lr_preds)
roc=roc_auc_score(y_test, lr_preds)

print(f"accuracy: {score:.3f}")
print(f"roc_auc_score: {roc:.3f}")
```

```python
from sklearn.model_selection import GridSearchCV

params = {'penalty': ['l2', 'l1'], 'C':[0.01, 0.1, 1, 1, 5, 10]}

grid_clf = GridSearchCV(lr_clf, param_grid=params , scoring="accuracy", cv=3)
grid_clf.fit(data_scaled, cancer.target)
best_params = grid_clf.best_params_
best_score = grid_clf.best_score_
print(f"최적 하이퍼 파라미터 :{best_params:}, 최적 평균 정확도: {best_score:.3f}")
```

```python
from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
import pandas as pd
import numpy as np
# 보스턴 데이터 세트로드
boston=load_boston()
bostonDF=pd.DataFrame(boston.data,columns=boston.feature_names)
from sklearn.ensemble import RandomForestRegressor
bostonDF['PRICE']=boston.target
y_target=bostonDF['PRICE']
X_data=bostonDF.drop(['PRICE'],axis=1,inplace=False)

rf=RandomForestRegressor(random_state=100,n_estimators=1000)
neg_mse_scores=cross_val_score(rf,X_data,y_target,scoring='neg_mean_squared_error')
rmse_scores=np.sqrt(-1*neg_mse_scores)
avg_rmse=np.mean(rmse_scores)
print('5 교차 검증의 개별 Negative MSE scores:',np.round(neg_mse_scores,2))
print('5 교차 검증의 개별 RMSE scroes : ',np.round(rmse_scores,2))
print('5 교차 검증의 평균 RMSE : {0:.3f}'.format(avg_rmse))
```

```python
def get_model_cv_prediction(model=None, X_data=None, y_target=None) :
    neg_mse_scores = cross_val_score(model, X_data, y_target, scoring="neg_mean_squared_error", cv=5)
    rmse_scores = np.sqrt(-1*neg_mse_scores)
    avg_rmse = np.mean(rmse_scores)
    if model == dt_reg :
        print(f"### DecisionTreeRegressor ### \n5 교차 검증의 평균 RMSE : {np.round(avg_rmse,3)}")
    elif model == rf_reg :
        print(f"### RandomForestRegressor ### \n5 교차 검증의 평균 RMSE : {np.round(avg_rmse,3)}")
    else :
        print(f"### XGBRegressor ### \n5 교차 검증의 평균 RMSE : {np.round(avg_rmse,3)}")
```

```python
from sklearn.tree import DecisionTreeRegressor
import xgboost as xgb
from xgboost import plot_importance
from xgboost import XGBRegressor

dt_reg = DecisionTreeRegressor(random_state=0, max_depth=4)
rf_reg = RandomForestRegressor(random_state = 0, n_estimators = 1000)

xgb_reg = XGBRegressor(random_state=0, max_depth=4)

models = [dt_reg, xgb_reg]
for model in models:
    get_model_cv_prediction(model, X_data, y_target)
```

```python
import seaborn as sns 
rf_reg = RandomForestRegressor(n_estimators=1000)
rf_reg.fit(X_data, y_target)

feature_series = pd.Series(data=rf_reg.feature_importances_, index = X_data.columns)
feature_series = feature_series.sort_values(ascending=False)
sns.barplot(x=feature_series, y = feature_series.index)
```

![output.png](%E1%84%85%E1%85%A9%E1%84%8C%E1%85%B5%E1%84%89%E1%85%B3%E1%84%90%E1%85%B5%E1%86%A8%20%E1%84%92%E1%85%AC%E1%84%80%E1%85%B1%201a68950519dc4c75a98b9ad4bbc1319e/output.png)